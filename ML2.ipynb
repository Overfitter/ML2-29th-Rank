{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\keras_tf\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import xgboost as xgb\n",
    "\n",
    "pd.set_option('display.max_colwidth',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #load goal data\n",
    "# train_goal = pd.read_csv('train_goal.csv')\n",
    "# test_goal = pd.read_csv('test_goal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert unix time format\n",
    "unix_cols = ['deadline','state_changed_at','launched_at','created_at']\n",
    "\n",
    "for x in unix_cols:\n",
    "    train[x] = train[x].apply(lambda k: datetime.datetime.fromtimestamp(int(k)).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    test[x] = test[x].apply(lambda k: datetime.datetime.fromtimestamp(int(k)).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_to_use = ['name','desc']\n",
    "len_feats = ['name_len','desc_len']\n",
    "count_feats = ['name_count','desc_count']\n",
    "\n",
    "for i in np.arange(2):\n",
    "    train[len_feats[i]] = train[cols_to_use[i]].apply(str).apply(len)\n",
    "    test[len_feats[i]] = test[cols_to_use[i]].apply(str).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['name_count'] = train['name'].str.split().str.len()\n",
    "train['desc_count'] = train['desc'].str.split().str.len()\n",
    "\n",
    "test['name_count'] = test['name'].str.split().str.len()\n",
    "test['desc_count'] = test['desc'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['keywords_len'] = train['keywords'].str.len()\n",
    "train['keywords_count'] = train['keywords'].str.split('-').str.len()\n",
    "\n",
    "test['keywords_len'] = test['keywords'].str.len()\n",
    "test['keywords_count'] = test['keywords'].str.split('-').str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# converting string variables to datetime\n",
    "unix_cols = ['deadline','state_changed_at','launched_at','created_at']\n",
    "\n",
    "for x in unix_cols:\n",
    "    train[x] = train[x].apply(lambda k: datetime.datetime.strptime(k, '%Y-%m-%d %H:%M:%S'))\n",
    "    test[x] = test[x].apply(lambda k: datetime.datetime.strptime(k, '%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# there should be simpler way - might take longer\n",
    "# creating list with time difference between 1) launched_at and created_at 2) deadline and launched_at\n",
    "\n",
    "time1 = []\n",
    "time3 = []\n",
    "for i in np.arange(train.shape[0]):\n",
    "    time1.append(np.round((train.loc[i, 'launched_at'] - train.loc[i, 'created_at']).total_seconds()).astype(int))\n",
    "    time3.append(np.round((train.loc[i, 'deadline'] - train.loc[i, 'launched_at']).total_seconds()).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['time1'] = np.log(time1)\n",
    "train['time3']= np.log(time3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for test data\n",
    "time5 = []\n",
    "time6 = []\n",
    "for i in np.arange(test.shape[0]):\n",
    "    time5.append(np.round((test.loc[i, 'launched_at'] - test.loc[i, 'created_at']).total_seconds()).astype(int))\n",
    "    time6.append(np.round((test.loc[i, 'deadline'] - test.loc[i, 'launched_at']).total_seconds()).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['time1'] = np.log(time5)\n",
    "test['time3'] = np.log(time6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat = ['disable_communication','country']\n",
    "\n",
    "for x in feat:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(train[x].values) + list(test[x].values))\n",
    "    train[x] = le.transform(list(train[x]))\n",
    "    test[x] = le.transform(list(test[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = pd.merge(train, train_goal, on='project_id', how='outer')\n",
    "# test = pd.merge(test, test_goal, on='project_id', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['goal'] = np.log1p(train['goal'])\n",
    "test['goal'] = np.log1p(test['goal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creating a full list of descriptions from train and etst\n",
    "kickdesc = pd.Series(train['desc'].tolist() + test['desc'].tolist()).astype(str)\n",
    "kickname = pd.Series(train['name'].tolist() + test['name'].tolist()).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this function cleans punctuations, digits and irregular tabs. Then converts the sentences to lower\n",
    "def desc_clean(word):\n",
    "    p1 = re.sub(pattern='(\\W+)|(\\d+)|(\\s+)',repl=' ',string=word)\n",
    "    p1 = p1.lower()\n",
    "    return p1\n",
    "\n",
    "kickdesc = kickdesc.map(desc_clean)\n",
    "kickname = kickname.map(desc_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append('C:\\\\Users\\\\user\\\\Anaconda3\\\\envs\\\\keras_tf\\\\Lib\\\\site-packages\\\\nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "kickdesc = [[x for x in x.split() if x not in stop] for x in kickdesc]\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "kickdesc = [[stemmer.stem(x) for x in x] for x in kickdesc]\n",
    "\n",
    "kickdesc = [[x for x in x if len(x) > 2] for x in kickdesc]\n",
    "\n",
    "kickdesc = [' '.join(x) for x in kickdesc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "kickname = [[x for x in x.split() if x not in stop] for x in kickname]\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "kickname = [[stemmer.stem(x) for x in x] for x in kickname]\n",
    "\n",
    "kickname = [[x for x in x if len(x) > 2] for x in kickname]\n",
    "\n",
    "kickname = [' '.join(x) for x in kickname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Due to memory error, limited the number of features to 650\n",
    "cv = CountVectorizer(max_features=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alldesc = cv.fit_transform(kickdesc).todense()\n",
    "allname = cv.fit_transform(kickname).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a data frame\n",
    "combine = pd.DataFrame(alldesc)\n",
    "combine.rename(columns= lambda x: 'variable_'+ str(x), inplace=True)\n",
    "combine1 = pd.DataFrame(allname)\n",
    "combine1.rename(columns= lambda x: 'variable_'+ str(x), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split the text features\n",
    "\n",
    "train_text = combine[:train.shape[0]]\n",
    "test_text = combine[train.shape[0]:]\n",
    "\n",
    "train_text1 = combine1[:train.shape[0]]\n",
    "test_text1 = combine1[train.shape[0]:]\n",
    "\n",
    "test_text.reset_index(drop=True,inplace=True)\n",
    "test_text1.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_to_use = ['name_len','desc_len','keywords_len','name_count','desc_count','keywords_count','time1','time3','goal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = train['final_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.loc[:,cols_to_use]\n",
    "test = test.loc[:,cols_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.concat([train, train_text,train_text1],axis=1)\n",
    "X_test = pd.concat([test, test_text,test_text1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108129, 1409)\n",
      "(63465, 1409)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(data=X_train, label = target)\n",
    "dtest = xgb.DMatrix(data=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective':'binary:logistic',\n",
    "    'eval_metric':'error',\n",
    "    'eta':0.025,\n",
    "    'max_depth':6,\n",
    "    'subsample':0.7,\n",
    "    'colsample_bytree':0.7,\n",
    "    'min_child_weight':5\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can probably get better accuracy with rounds > 1000. \n",
    "bst = xgb.cv(params, dtrain, num_boost_round=1000, early_stopping_rounds=40,nfold=5,verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst_train = xgb.train(params, dtrain, num_boost_round=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_test = bst_train.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['project_id'] = test['project_id']\n",
    "sub['final_status'] = p_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub['final_status'] = [1 if x > 0.5 else 0 for x in sub['final_status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv(\"xgb_with_python_feats.csv\",index=False) #0.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "train_data = lgb.Dataset(X_train.values, label=target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setting parameters for lightgbm\n",
    "param = {'num_leaves':100, 'objective':'binary','max_depth':10,'learning_rate':0.05,'max_bin':500}\n",
    "param['metric'] = ['accuracy']\n",
    "#depth8,0.05,30\n",
    "#10,0.05,100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training our model using light gbm\n",
    "num_round=2000\n",
    "# start=datetime.now()\n",
    "lgbm=lgb.train(param,train_data,num_round)\n",
    "# stop=datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = lgbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test1 = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['project_id'] = test1['project_id']\n",
    "sub['final_status'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub['final_status'] = [1 if x > 0.5 else 0 for x in sub['final_status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv(\"lgbm_with_python_extrafeats3.csv\",index=False) #0.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ensemble\n",
    "y_ens = 0.6*y_pred + 0.4*p_test\n",
    "sub = pd.DataFrame()\n",
    "sub['project_id'] = test1['project_id']\n",
    "sub['final_status'] = y_ens\n",
    "sub['final_status'] = [1 if x > 0.5 else 0 for x in sub['final_status']]\n",
    "sub.to_csv(\"lgbm_xgb_4.csv\",index=False) #0.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\keras_tf\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['test']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.misc import imread\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To stop potential randomness\n",
    "seed = 128\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = X_train.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_size = int(X_train.shape[0]*0.7)\n",
    "\n",
    "train_x, val_x = train_x[:split_size], train_x[split_size:]\n",
    "train_y, val_y = target.values[:split_size], target.values[split_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_to_one_hot(labels_dense, num_classes=10):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = numpy.arange(num_labels) * num_classes\n",
    "    labels_one_hot = numpy.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    \n",
    "    return labels_one_hot\n",
    "\n",
    "def preproc(unclean_batch_x):\n",
    "    \"\"\"Convert values to range 0-1\"\"\"\n",
    "    temp_batch = unclean_batch_x / unclean_batch_x.max()\n",
    "    \n",
    "    return temp_batch\n",
    "\n",
    "def batch_creator(batch_size, dataset_length, dataset_name):\n",
    "    \"\"\"Create batch with random samples and return appropriate format\"\"\"\n",
    "    batch_mask = rng.choice(dataset_length, batch_size)\n",
    "    \n",
    "    batch_x = eval(dataset_name + '_x')[[batch_mask]].reshape(-1, 784)\n",
    "    batch_x = preproc(batch_x)\n",
    "    \n",
    "    if dataset_name == 'train':\n",
    "        batch_y = eval(dataset_name).ix[batch_mask, 'label'].values\n",
    "        batch_y = dense_to_one_hot(batch_y)\n",
    "        \n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of neurons in each layer\n",
    "input_num_units = 1409\n",
    "hidden_num_units = 500\n",
    "output_num_units = 2\n",
    "\n",
    "# define placeholders, i.e. way to feed values to computational graph\n",
    "x = tf.placeholder(tf.float32, [None, input_num_units])\n",
    "y = tf.placeholder(tf.float32, [None, output_num_units])\n",
    "\n",
    "# set remaining parameters\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([input_num_units, hidden_num_units], seed=seed)),\n",
    "    'output': tf.Variable(tf.random_normal([hidden_num_units, output_num_units], seed=seed))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([hidden_num_units], seed=seed)),\n",
    "    'output': tf.Variable(tf.random_normal([output_num_units], seed=seed))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_layer = tf.add(tf.matmul(x, weights['hidden']), biases['hidden'])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "output_layer = tf.matmul(hidden_layer, weights['output']) + biases['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=output_layer,logits= y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'Variable:0' shape=(1409, 800) dtype=float32_ref>\", \"<tf.Variable 'Variable_1:0' shape=(800, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_2:0' shape=(800,) dtype=float32_ref>\", \"<tf.Variable 'Variable_3:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'Variable_4:0' shape=(1409, 500) dtype=float32_ref>\", \"<tf.Variable 'Variable_5:0' shape=(500, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_6:0' shape=(500,) dtype=float32_ref>\", \"<tf.Variable 'Variable_7:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'Variable_8:0' shape=(1409, 1000) dtype=float32_ref>\", \"<tf.Variable 'Variable_9:0' shape=(1000, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_10:0' shape=(1000,) dtype=float32_ref>\", \"<tf.Variable 'Variable_11:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'Variable_12:0' shape=(1409, 1000) dtype=float32_ref>\", \"<tf.Variable 'Variable_13:0' shape=(1000, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_14:0' shape=(1000,) dtype=float32_ref>\", \"<tf.Variable 'Variable_15:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'Variable_16:0' shape=(1409, 1200) dtype=float32_ref>\", \"<tf.Variable 'Variable_17:0' shape=(1200, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_18:0' shape=(1200,) dtype=float32_ref>\", \"<tf.Variable 'Variable_19:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'Variable_20:0' shape=(1409, 1300) dtype=float32_ref>\", \"<tf.Variable 'Variable_21:0' shape=(1300, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_22:0' shape=(1300,) dtype=float32_ref>\", \"<tf.Variable 'Variable_23:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'Variable_24:0' shape=(1409, 1000) dtype=float32_ref>\", \"<tf.Variable 'Variable_25:0' shape=(1000, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_26:0' shape=(1000,) dtype=float32_ref>\", \"<tf.Variable 'Variable_27:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'dense_1/kernel:0' shape=(1409, 150) dtype=float32_ref>\", \"<tf.Variable 'dense_1/bias:0' shape=(150,) dtype=float32_ref>\", \"<tf.Variable 'dense_2/kernel:0' shape=(150, 2) dtype=float32_ref>\", \"<tf.Variable 'dense_2/bias:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'iterations:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'lr:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'beta_1:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'beta_2:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'decay:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'dense_3/kernel:0' shape=(1409, 150) dtype=float32_ref>\", \"<tf.Variable 'dense_3/bias:0' shape=(150,) dtype=float32_ref>\", \"<tf.Variable 'dense_4/kernel:0' shape=(150, 2) dtype=float32_ref>\", \"<tf.Variable 'dense_4/bias:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'iterations_1:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'lr_1:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'beta_1_1:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'beta_2_1:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'decay_1:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'dense_5/kernel:0' shape=(1409, 150) dtype=float32_ref>\", \"<tf.Variable 'dense_5/bias:0' shape=(150,) dtype=float32_ref>\", \"<tf.Variable 'dense_6/kernel:0' shape=(150, 1) dtype=float32_ref>\", \"<tf.Variable 'dense_6/bias:0' shape=(1,) dtype=float32_ref>\", \"<tf.Variable 'iterations_2:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'lr_2:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'beta_1_2:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'beta_2_2:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'decay_2:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'dense_7/kernel:0' shape=(1409, 150) dtype=float32_ref>\", \"<tf.Variable 'dense_7/bias:0' shape=(150,) dtype=float32_ref>\", \"<tf.Variable 'dense_8/kernel:0' shape=(150, 2) dtype=float32_ref>\", \"<tf.Variable 'dense_8/bias:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'iterations_3:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'lr_3:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'beta_1_3:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'beta_2_3:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'decay_3:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'dense_9/kernel:0' shape=(1409, 150) dtype=float32_ref>\", \"<tf.Variable 'dense_9/bias:0' shape=(150,) dtype=float32_ref>\", \"<tf.Variable 'dense_10/kernel:0' shape=(150, 2) dtype=float32_ref>\", \"<tf.Variable 'dense_10/bias:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'iterations_4:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'lr_4:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'beta_1_4:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'beta_2_4:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'decay_4:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'Variable_28:0' shape=(1409, 500) dtype=float32_ref>\", \"<tf.Variable 'Variable_29:0' shape=(500, 10) dtype=float32_ref>\", \"<tf.Variable 'Variable_30:0' shape=(500,) dtype=float32_ref>\", \"<tf.Variable 'Variable_31:0' shape=(10,) dtype=float32_ref>\", \"<tf.Variable 'Variable_32:0' shape=(1409, 500) dtype=float32_ref>\", \"<tf.Variable 'Variable_33:0' shape=(500, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_34:0' shape=(500,) dtype=float32_ref>\", \"<tf.Variable 'Variable_35:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'Variable_36:0' shape=(1409, 500) dtype=float32_ref>\", \"<tf.Variable 'Variable_37:0' shape=(500, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_38:0' shape=(500,) dtype=float32_ref>\", \"<tf.Variable 'Variable_39:0' shape=(2,) dtype=float32_ref>\"] and loss Tensor(\"Mean_31:0\", shape=(), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-5e8da1634f64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_tf\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[0;32m    320\u001b[0m           \u001b[1;34m\"No gradients provided for any variable, check your graph for ops\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m           \u001b[1;34m\" that do not support gradients, between variables %s and loss %s.\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m           ([str(v) for _, v in grads_and_vars], loss))\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'Variable:0' shape=(1409, 800) dtype=float32_ref>\", \"<tf.Variable 'Variable_1:0' shape=(800, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_2:0' shape=(800,) dtype=float32_ref>\", \"<tf.Variable 'Variable_3:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'Variable_4:0' shape=(1409, 500) dtype=float32_ref>\", \"<tf.Variable 'Variable_5:0' shape=(500, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_6:0' shape=(500,) dtype=float32_ref>\", \"<tf.Variable 'Variable_7:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'Variable_8:0' shape=(1409, 1000) dtype=float32_ref>\", \"<tf.Variable 'Variable_9:0' shape=(1000, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_10:0' shape=(1000,) dtype=float32_ref>\", \"<tf.Variable 'Variable_11:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'Variable_12:0' shape=(1409, 1000) dtype=float32_ref>\", \"<tf.Variable 'Variable_13:0' shape=(1000, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_14:0' shape=(1000,) dtype=float32_ref>\", \"<tf.Variable 'Variable_15:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'Variable_16:0' shape=(1409, 1200) dtype=float32_ref>\", \"<tf.Variable 'Variable_17:0' shape=(1200, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_18:0' shape=(1200,) dtype=float32_ref>\", \"<tf.Variable 'Variable_19:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'Variable_20:0' shape=(1409, 1300) dtype=float32_ref>\", \"<tf.Variable 'Variable_21:0' shape=(1300, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_22:0' shape=(1300,) dtype=float32_ref>\", \"<tf.Variable 'Variable_23:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'Variable_24:0' shape=(1409, 1000) dtype=float32_ref>\", \"<tf.Variable 'Variable_25:0' shape=(1000, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_26:0' shape=(1000,) dtype=float32_ref>\", \"<tf.Variable 'Variable_27:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'dense_1/kernel:0' shape=(1409, 150) dtype=float32_ref>\", \"<tf.Variable 'dense_1/bias:0' shape=(150,) dtype=float32_ref>\", \"<tf.Variable 'dense_2/kernel:0' shape=(150, 2) dtype=float32_ref>\", \"<tf.Variable 'dense_2/bias:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'iterations:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'lr:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'beta_1:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'beta_2:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'decay:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'dense_3/kernel:0' shape=(1409, 150) dtype=float32_ref>\", \"<tf.Variable 'dense_3/bias:0' shape=(150,) dtype=float32_ref>\", \"<tf.Variable 'dense_4/kernel:0' shape=(150, 2) dtype=float32_ref>\", \"<tf.Variable 'dense_4/bias:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'iterations_1:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'lr_1:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'beta_1_1:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'beta_2_1:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'decay_1:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'dense_5/kernel:0' shape=(1409, 150) dtype=float32_ref>\", \"<tf.Variable 'dense_5/bias:0' shape=(150,) dtype=float32_ref>\", \"<tf.Variable 'dense_6/kernel:0' shape=(150, 1) dtype=float32_ref>\", \"<tf.Variable 'dense_6/bias:0' shape=(1,) dtype=float32_ref>\", \"<tf.Variable 'iterations_2:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'lr_2:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'beta_1_2:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'beta_2_2:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'decay_2:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'dense_7/kernel:0' shape=(1409, 150) dtype=float32_ref>\", \"<tf.Variable 'dense_7/bias:0' shape=(150,) dtype=float32_ref>\", \"<tf.Variable 'dense_8/kernel:0' shape=(150, 2) dtype=float32_ref>\", \"<tf.Variable 'dense_8/bias:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'iterations_3:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'lr_3:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'beta_1_3:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'beta_2_3:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'decay_3:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'dense_9/kernel:0' shape=(1409, 150) dtype=float32_ref>\", \"<tf.Variable 'dense_9/bias:0' shape=(150,) dtype=float32_ref>\", \"<tf.Variable 'dense_10/kernel:0' shape=(150, 2) dtype=float32_ref>\", \"<tf.Variable 'dense_10/bias:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'iterations_4:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'lr_4:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'beta_1_4:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'beta_2_4:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'decay_4:0' shape=() dtype=float32_ref>\", \"<tf.Variable 'Variable_28:0' shape=(1409, 500) dtype=float32_ref>\", \"<tf.Variable 'Variable_29:0' shape=(500, 10) dtype=float32_ref>\", \"<tf.Variable 'Variable_30:0' shape=(500,) dtype=float32_ref>\", \"<tf.Variable 'Variable_31:0' shape=(10,) dtype=float32_ref>\", \"<tf.Variable 'Variable_32:0' shape=(1409, 500) dtype=float32_ref>\", \"<tf.Variable 'Variable_33:0' shape=(500, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_34:0' shape=(500,) dtype=float32_ref>\", \"<tf.Variable 'Variable_35:0' shape=(2,) dtype=float32_ref>\", \"<tf.Variable 'Variable_36:0' shape=(1409, 500) dtype=float32_ref>\", \"<tf.Variable 'Variable_37:0' shape=(500, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_38:0' shape=(500,) dtype=float32_ref>\", \"<tf.Variable 'Variable_39:0' shape=(2,) dtype=float32_ref>\"] and loss Tensor(\"Mean_31:0\", shape=(), dtype=float32)."
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define vars\n",
    "input_num_units = 1409\n",
    "hidden_num_units = 500\n",
    "output_num_units = 2\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\keras_tf\\lib\\site-packages\\ipykernel\\__main__.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=1409, units=500)`\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\user\\Anaconda3\\envs\\keras_tf\\lib\\site-packages\\ipykernel\\__main__.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", input_dim=500, units=2)`\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(output_dim=hidden_num_units, input_dim=input_num_units, activation='relu'),\n",
    "        \n",
    "    Dense(output_dim=output_num_units, input_dim=hidden_num_units, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\keras_tf\\lib\\site-packages\\keras\\models.py:851: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_12 to have shape (None, 2) but got array with shape (75690, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-153-5b956f60ef04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrained_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_tf\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    868\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1438\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1317\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1319\u001b[1;33m                                     exception_prefix='target')\n\u001b[0m\u001b[0;32m   1320\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[0;32m   1321\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m                             \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m                             str(array.shape))\n\u001b[0m\u001b[0;32m    140\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_12 to have shape (None, 2) but got array with shape (75690, 1)"
     ]
    }
   ],
   "source": [
    "trained_model = model.fit(train_x, train_y, nb_epoch=epochs, batch_size=batch_size, validation_data=(val_x, val_y))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:keras_tf]",
   "language": "python",
   "name": "conda-env-keras_tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
